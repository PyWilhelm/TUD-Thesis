\section{Experimental Results and Discussion}
\label{sec:5}

The experimental results are described, analyzed and concluded in this section. In section \ref{sec:5.1} the preprocessing methods are analyzed through comparing the statistics of the pre-processed corpus. Section \ref{sec:5.2} focuses on the effectiveness of the different STS models, i.e. how precisely the STS models judge articles as related to the given target article. Furthermore, the operational performance of the STS models is reported in section \ref{sec:5.3}. The above three parts concentrate on the results of the first experiment, which is mentioned in section \ref{sec:4.4}. On the next step, the results of the second experiment are reported in section \ref{sec:5.4}. After reporting the results of the both experiments, we discuss the most severe types of errors that an article which is virtually unrelated is judged as related to a target article or a cluster of articles which are properly related are never or rarely judged as related. In conclusion, we summarize the consequence of the selection of the STS models and the challenge of the current work. 

\bigbreak

\subsection{Analysis of Preprocessing Methods}
\label{sec:5.1}

The text fields of each article including \icontent{}, \ititle{} and \isummary{} are used as the semantic input data of the systems. After preprocessing, the text fields in raw string format are converted to sequences of words in which each works consist of $n$ tokens in the n-gram approach. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/freqdist}
    \caption{Occurrence Frequencies of terms in the corpus for uni-, bi- and trigram in descending ranking. The applied preprocessing method is \iSE{} and the text source is \icontent{}.}
    \label{fig:freqdist}
\end{figure}



There are two attributes to portray a vocabulary. The first attribute is the size, which decides the computation complexity, such as the dimension of vector space. The second one is the scale of ``long tail'' of the vocabulary. The ``long tail'' is the part of the distribution which contains a large number of elements with low occurrence frequency. In our case, the threshold of ``long tail'' is $5$, in other words, the terms occurring less than $5$ times belong to the ``long tail'' of the vocabulary. According to the Zipf's law, the occurrence frequency is distributed extremely unevenly, that is to say, the most terms are rarely in use and hence it is almost incapable of determining the semantic relevance of these terms by statistical methods. In the ideal case, a vocabulary with the smaller size and fewer low-frequency terms has the better quality of operational complexity and semantic completeness. Figure \ref{fig:freqdist}, which depicts the representative distribution of the occurrence frequency of terms, indicates that only a small quantity of tokens occur repetitively in the corpus (25\% for unigram, 10\% for bigram and 3\% for trigram). In respect of efficiency, the tokens which appear less than $5$ times can be removed from the vocabulary. We denote the original vocabularies as \ifull{} vocabularies and the reduced vocabularies as \icommon{} vocabularies. 



We consider the size of \ifull{} vocabularies and the proportion of frequent tokens as the metrics to evaluate the vocabulary quality. Figure \ref{fig:vocab_size} shows the comparison between the size of \ifull{} and \icommon{} vocabularies which are generated by the preprocessing methods including \iSP{}, \iSE{}, \iST{} and \iSS{} over the different n-gram models ($n=1, 2, 3$) of the three types of text fields containing \icontent{}, \ititle{} and \isummary{}. In the unigram model, \iSS{} always generates the vocabularies with the smallest size and the largest proportion of frequent terms. In bi- and trigram, the generated by \iSE{} vocabularies have the largest proportion of frequent terms and the size is slightly different with the vocabularies which are generated by other methods. In conclusion, preprocessing method \iSS{} is applied for preprocessing using the unigram model and \iSE{} is applied for preprocessing using the bigram and trigram models in the application. 


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/vocab_size}
    \begin{tabular}{|ll|} \hline \textbf{Legend} & \\ Column & types of text fields; \icontent{}, \ititle{} and \isummary{}, respectively \\ Row & corresponding n-gram models; uni-, bi-, trigram, respectively \\ Blue bars & \ifull{} vocabularies containing all terms \\ Green bars & \icommon{} vocabularies retaining frequently used terms \\ Red lines & the ratio of the size of \icommon{} vocabularies to the size of the corresponding \\ & \ifull{} vocabularies. \\ \hline \end{tabular}
    \caption{Compare the vocabulary size of all text fields over n-gram models.}
    \label{fig:vocab_size}
\end{figure}

\subsection{Effectiveness of STS Models}
\label{sec:5.2}


As described in section \ref{sec:3.3}, the indicators of effectiveness are \textit{precision@k@h} to evaluate the relatedness of the articles in the top positions and \textit{nDCG} to evaluate the overall results. First, the results of the STS methods over each processed field are given and the best STS methods are selected. We give in advance the reasonable assumption that an article is definitively (or ``true'') related to the articles which are not further than $3$ hops in the related-graph. Later in this part, the divergence between different categories is discussed and for which categories the framework is more suitable are concluded.  


\subsubsection{Parameter Selection for Topic Models}

Topic models, such as LSI and LDA, represent documents as vectors in the topic space. The dimension of the topic space, as well as the number of topics, is pre-defined. Hence, it is necessary to determine the optimal number of topics before building and evaluating models. Figure \ref{fig:precision_topics} shows the plot of precision and operational time of LSI along with increasing the number of topics in the range of $50 \sim 1000$. The systems are generated with $73908$ candidate articles and then discover related articles for $2000$ randomly selected articles. In the left subfigure, the plots of \textit{precision@2@3} for the text fields over unigram are drawn. In the right subfigure, the dash lines indicate the building time and the solid lines indicate the sum of building time and total predicting time. On one side, the precision keeps monotone increasing  but the growth rate decreases, as the number of topics becomes larger. On the other side, the time cost of building and predicting increases linearly. Taking the limitation of time of running the experiments into account, we select $500$ as the value of this hyperparameter. The reason is the growth rate of precision is lower than $0.4\%$ per $100$ topics if the number of topics is more than $500$, and we expect that the building time is limited to one hour and the predicting time is up to less than one second. 

\clearpage

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/precision_topics}
    \caption[Compare the effectiveness and efficiency of the systems using LSI with different topic numbers from $50$ to $1000$.]{Compare the effectiveness and efficiency of the systems using LSI with different topic numbers from $50$ to $1000$. The left subfigure illustrates  \textit{precision@2@3} for all text fields over unigram and the right subfigure the time cost in model building and in the entire process. }
    \label{fig:precision_topics}
\end{figure}

\subsubsection{Overall Analysis of Effectiveness}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/precision_2_3}
    \caption[Compare \textit{precision@2@3} (\%) of Jaccard, BoW, \tfidf{}, LSI and LDA for all text fields over n-gram models.]{Compare \textit{precision@2@3} (\%) of Jaccard, BoW, \tfidf{}, LSI and LDA for all text fields over n-gram models. The values with red color refer to the precision of the best STS methods for the processed field. }
    \label{fig:precision_2_3}
\end{figure}

Figure \ref{fig:precision_2_3} shows the comparison of the precision of the STS methods over the different processed fields. In general, all methods perform in \icontent{} better than in \ititle{} and the methods perform in \ititle{} perform better than in \isummary{}. Besides, the precision of the systems using the unigram model is better than using the bigram model and the precision of the system using trigram is the worst. In more detail, \tfidf{} provides the highest precision for all text fields over unigram and for \icontent{} and \ititle{} over bigram. In the meanwhile, Jaccard is stronger than the other STS methods for \isummary{} in bigram and all text fields in trigram. LDA is unexpectedly the weakest method all the time in the experiment and it is discarded for evaluation in bigram and trigram. Comparing the different n-gram models for the particular text fields, we find that the precision of \textit{Jaccard} fluctuates most slightly, and in contrast, the precision of LSI decreases with the greatest rate. 

Figure \ref{fig:ndcg} illustrates the comparison of \textit{nDCG}, which is applied as the supplementary evaluation method. The ranking of \textit{nDCG} of the STS methods is similar to the results of \textit{precision} in general. We observe that measure \textit{nDCG} of every method is better than the baseline. Therefore, all STS methods are capable of effective sorting articles according to the relatedness degree. The baseline is computed from a random order of all candidates. In more detail, LSI performs better in measure \textit{nDCG} than in measure \textit{precision} for \icontent{}. To be specific, LSI is almost always in the second position after \tfidf{} in \textit{nDCG}, while LSI is in the third or the last position in \textit{precision}. LSI computes the more precise relatedness of the overall corpus than \textit{Jaccard} and BoW, but LSI is not able to determine precisely which articles have the highest relatedness. 


Combining the results of two evaluation methods, we conclude as follows.

\begin{itemize}
\item The methods using short documents, such as \ititle{} are worse than the methods using long documents like \icontent{}. 

\item \textit{Title} performs better than \isummary{} in the semantic similarity computation.

\item In n-gram, the vocabulary becomes larger and the relevance between phrases become weaker along with a greater $n$. On the other hand, a higher $n$ leads to the greater loss of semantic information with removing the low-frequency terms from the vocabulary. For instance, $77\%$ information/phrases are lost through reducing vocabulary for \icontent{} over trigram. 

\item \tfidf{} is in the dominant position for uni-/bigram and long documents, and Jaccard is more suitable in trigram and short documents. LDA has the unexpected unsatisfied performance.
\end{itemize}
 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/ndcg}
    \caption[Compare \textit{nDCG} of Jaccard, BoW, \textit{tfidf}, LSI and LDA for all text fields over n-gram models]{Compare \textit{nDCG} of Jaccard, BoW, \textit{tfidf}, LSI and LDA for all text fields over n-gram models. The legend is the same to figure \ref{fig:precision_2_3}}
    \label{fig:ndcg}
\end{figure}

\subsubsection{Coverage of Predictions in Related-Graph}

In this part, we pay attention to how the selections of related articles by the different STS methods are distributed over the related-graph. We analyze the performance of the STS methods in evaluation measure \textit{precision@2@h}, where $h$ can be $1$, $2$, $3$, $5$, $8$ and $10$. Here we only discuss the performance over \icontent{}, because the methods over other text fields cannot be applied alone in the system due to the low precision. 

Table \ref{tab:hops} reports \textit{precision@2@h} of the STS methods for \icontent{} over the n-gram models. The random baseline is the precision of random selections. The baseline also indicates the average proportion of the number of $h$-hop neighbors for each target article to the size of candidate dataset. Taking $10$-hops as an example, $21.64\%$ articles in the candidate corpus are related in $10$-hops to a target article in average. For unigram, the best method for $10$-hop is LSI, of which precision is about 4 times as much as the baseline. In addition, the ranking of LSI increases along with increasing $h$. More detailed, LSI takes the fourth place of all methods for $1$-hop in all n-gram models, and it wins the first, second and third place for $10$-hop in uni-, bi- and trigram, respectively. Integrating such results with the analysis of nDCG, we consider that LSI is good at distinguishing related articles from unrelated articles but not so good at differentiating the relatedness degree of the related articles. 

\begin{table}[!htb]
\centering
\small
\begin{tabularx}{\textwidth}{XX|XXXXXX}
\textbf{n-gram} & \textbf{STS} & \textbf{1-hop} & \textbf{2-hop} & \textbf{3-hop} & \textbf{5-hop} & \textbf{8-hop} & \textbf{10-hop} \\ \hline

\multicolumn{2}{c|}{\textbf{Random Baseline}} & 6.0E-5& 2.2E-4 & 6.8E-4 & ~~0.47 & ~~5.65 & 21.64 \\ \hline

\multirow{5}{*}{\textbf{unigram}} & Jaccard & 19.68 & 31.90 & 39.45 & 51.63 & 69.28 & 80.15 \\
 & BoW & 19.80 & 32.55 & 40.43 & 53.33 & 71.00 & 81.28 \\
 & tf-idf & \textbf{22.55} & \textbf{37.25} & \textbf{45.08} & \textbf{57.90} & 73.45 & 82.33 \\
 & LSI & 17.33 & 29.83 & 38.58 & 53.35 & \textbf{73.93} & \textbf{84.40} \\
 & LDA & ~~5.80 & 11.08 & 15.55 & 26.08 & 45.50 & 62.48 \\ \hline
\multirow{4}{*}{\textbf{bigram}} & Jaccard & 17.53 & 29.43 & 36.10 & 47.28 & 63.23 & 74.50 \\
 & BoW & 12.13 & 20.75 & 25.90 & 35.68 & 50.90 & 64.58 \\
 & tf-idf & 21.50 & 35.00 & 42.65 & 56.20 & 71.93 & 81.70 \\
 & LSI & 11.53 & 20.65 & 27.38 & 42.28 & 64.40 & 77.63 \\ \hline
\multirow{4}{*}{\textbf{trigram}} & Jaccard & 14.25 & 23.83 & 29.03 & 38.33 & 52.80 & 64.48 \\
 & BoW & 10.95 & 18.48 & 23.18 & 31.83 & 46.85 & 60.15 \\
 & tf-idf & 13.48 & 22.93 & 28.75 & 39.60 & 56.00 & 68.30 \\
 & LSI & ~~4.63 & ~~9.78 & 14.28 & 24.25 & 43.60 & 60.88 \\ \hline
\end{tabularx}
\caption[\textit{Precision@2@h} (\%) of STS methods for \icontent{} over n-gram models, where $1, 2, 3, 5, 8 \text{and} 10$ can be assigned to $h$]{\textit{Precision@2@h} (\%) of STS methods for \icontent{} over n-gram models, where $1, 2, 3, 5, 8 \text{and} 10$ can be assigned to $h$. The random baseline is computed with random selection of the candidate corpus. Two candidates for each target are selected randomly and \textit{precision@2@h} is computed as the normal way. }
\label{tab:hops}
\end{table}

\subsubsection{Divergence between Categories}

\begin{table}[!htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrr|rr|rr|rr|rr|rr|rr|rr|rr}
 & \multicolumn{6}{c|}{\textbf{Content}} & \multicolumn{6}{c|}{\textbf{Title}} & \multicolumn{6}{c}{\textbf{Summary}} \\
 & \multicolumn{2}{c|}{\textbf{1-tfidf}} & \multicolumn{2}{c|}{\textbf{2-tfidf}} & \multicolumn{2}{c|}{\textbf{3-jaccard}} & \multicolumn{2}{c|}{\textbf{1-tfidf}} & \multicolumn{2}{c|}{\textbf{2-tfidf}} & \multicolumn{2}{c|}{\textbf{3-jaccard}} & \multicolumn{2}{c|}{\textbf{1-tfidf}} & \multicolumn{2}{c|}{\textbf{2-jaccard}} & \multicolumn{2}{c}{\textbf{3-jaccard}} \\
 & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c|}{\textbf{prec}} & \multicolumn{1}{c}{\textbf{r}} & \multicolumn{1}{c}{\textbf{prec}} \\ \hline
\textbf{Mean} & & 45.07 & & 42.65 & & 29.03 & & 30.85 & & 12.12 & & 8.65 & & 20.25 & & 8.20 & & 6.48 \\ \hline

\textbf{\textcolor{blue}{Digital}} & 1 & 52.21 & 2  & 46.90 & 7  & 19.03 & 1  & 38.50 & 5  & 13.72 & 7  & 6.64 & 3  & 22.12 & 6  & 6.64 & 8  & 4.42 \\
\textbf{\textcolor{blue}{Wissen}} & 2 & 50.98 & 3  & 46.08 & 4  & 23.86 & 2  & 35.62 & 9  & 7.52 & 9  & 6.54 & 7  & 17.32 & 9  & 5.56 & 10  & 3.59 \\
\textbf{\textcolor{blue}{Gesellschaft}} & 3  & 49.02 & 4  & 45.59 & 2  & 34.07 & 3  & 35.54 & 4  & 13.73 & 2  & 13.24 & 2  & 23.28 & 2  & 10.54 & 2  & 9.31 \\
\textbf{\textcolor{blue}{Politik}} & 4  & 48.79 & 1  & 48.86 & 1  & 41.18 & 4  & 33.00  & 6  & 12.73 & 5  & 9.25 & 1  & 23.54 & 3  & 9.53 & 3  & 7.18 \\
\textbf{Karriere} & 5  & 48.04 & 7  & 37.25 & 8  & 16.67 & 5  & 30.39 & 10  & 6.86 & 6  & 6.86 & 6  & 17.65 & 8  & 5.88 & 4  & 6.86 \\
\textbf{Kultur} & 6  & 42.69 & 6  & 38.58 & 9  & 13.93 & 6  & 29.91 & 1  & 17.58 & 4  & 9.36 & 4  & 21  & 4  & 9.36 & 5  & 6.85 \\
\textbf{Sport} & 7  & 41.80 & 5  & 38.93 & 5  & 23.77 & 7  & 25.82 & 3  & 16.39 & 8  & 6.56 & 5  & 18.44 & 5  & 7.79 & 6  & 6.15 \\
\textbf{Wirtschaft} & 8  & 38.44 & 8  & 35.18 & 3  & 25.24 & 8  & 24.92 & 8  & 7.65 & 10  & 6.35 & 8  & 17.10 & 7  & 6.51 & 7  & 5.70 \\
\textbf{\textcolor{red}{Reisen}} & 9  & 31.45 & 9  & 29.03 & 10  & 13.71 & 10  & 20.16 & 7  & 9.68 & 3  & 9.68 & 10  & 7.26 & 10  & 4.03 & 9  & 4.03 \\
\textbf{\textcolor{red}{Lebensart}} & 10  & 29.31 & 10  & 27.59 & 6  & 20.69 & 9  & 22.41 & 2  & 17.24 & 1  & 13.79 & 9  & 13.79 & 1  & 12.07 & 1  & 10.34 \\
\textbf{\textcolor{red}{Studium}} & 11  & 17.57 & 11  & 21.62 & 11  & 9.46 & 11  & 17.57 & 11  & 4.05 & 11  & 5.41 & 11  & 5.41 & 11  & 1.35 & 11  & 1.35 \\ \hline

\end{tabular}%
}
\caption[Comparison between \textit{precision@2@3} (\%) of different categories and the mean precision of the entire corpus]{Comparison between \textit{precision@2@3} (\%) of different categories and the mean precision of the entire corpus. \textbf{r} refers to the ranking of precision in all categories and \textbf{prec} indicates the precision for the specific category}
\label{tab:cate_precision}
\end{table}


The effectiveness is reported and analyzed above in general. Now we are also interested, how the methods, which are selected as the best for the preprocessed text fields, work for specific categories. The results are drawn in table \ref{tab:cate_precision}, where the precision ranking of each category and the difference ratio to the mean precision of the entire corpus are revealed. \textit{Gesellschaft} (\textit{society}) and \textit{Politik} (\textit{politics}) surpass the mean precision in all PFs. On the contrary, \textit{Reisen} (\textit{travel}) and \textit{Studium} (\textit{study}) are the worst performed categories. The precision of these two categories is much lower than the mean level. The systems using different configurations performs quite differently in category \textit{Digital} (\textit{digital}) and \textit{Lebensart} (\textit{lifestyle}) are have quite. For example, the precision of the best category (\textit{Digital}) in \icontent{} over unigram is about three times greater than the precision of the weakest category (\textit{Studium}), while the precision of \textit{Digital} in \isummary{} over bigram is just $40\%$ as much as the precision of the best configuration. The output results of the systems hence have different confidence degree according to the category which the target article belongs to. 

\subsection{Efficiency of STS Models}
\label{sec:5.3}

The complete workflow of the system can be simplified to two main phases, scilicet the building and predicting phases. In the building phase, the candidate dataset is processed or transformed through the sub-phases successively including preprocessing, vocabulary generating, model building and then instance transforming. After the building phase, articles from the candidate corpus are represented as vectors or term-sets. The predicting phase is the phase to discover related articles for target articles through three sub-phases: vector representation, \textit{cosine} similarity computation and ordering. 

The time cost of preprocessing is not taken into account since the cost is relatively constant and tiny, and more importantly, preprocessing does independent of the selection of STS methods nor n-gram models. In following, we analyze efficiency on three views: the internal comparison between STS methods, the vertical comparison between n-gram models, and the  horizontal comparison between text fields. 

\subsubsection{Comparison of building time}


The results of building time from experiment 1 are illustrated in figure \ref{fig:build_time} and analyzed as follows. 

\begin{itemize}
\item The building phase of \textit{Jaccard} spends less time than other VSMs. Unlike VSMs, there is no additional semantic model t while could be generated from candidate articles. In the building phase of \textit{Jaccard}, the vocabulary is generated and then every article is reduced into a set of used terms. 

\item The building time cost of BoW and \tfidf{} is roughly the same and \tfidf{} spends slightly more time than BoW since \tfidf{} is extended from BoW with computing \tfidf{} weight. 

\item Topic models are more expensive. Compared with \tfidf{}, LSI consumes more than $5 \times$ time for \icontent{} over unigram and even more than $20 \times$ time for \icontent{} over trigram due to SVD computation. Meanwhile, the building time of LDA is $47$ times as high as the building time of \tfidf{} for \icontent{} over unigram and the time cost exceeds the acceptable range in bi- and trigram. 

\item Compared with the time cost in unigram, the building time cost of \textit{Jaccard}, BoW, \tfidf{} and LSI for \icontent{} in bigram increases by 3.2 times, 2.1 times, 2.1 times and 4.6 times, respectively, and the time cost in trigram increases only by 2.4 times, 1.2 times, 1.2 times and 4.2 times. 

\end{itemize}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/building_time}
    \caption[Time cost (second) of building phase of Jaccard, BoW, \textit{tfidf}, LSI and LDA for all text fields over n-gram models]{Time cost (second) of building phase of Jaccard, BoW, \textit{tfidf}, LSI and LDA for all text fields over n-gram models. $73908$ candidate articles are used as training data to build the STS models.}
    \label{fig:build_time}
\end{figure}
\subsubsection{Comparison of predicting time}

The results of predicting time are illustrated in figure \ref{fig:predict_time} and analyzed as follows. 


\begin{itemize}
\item Similar to building phase, \textit{Jaccard} is also quite different from other VSM methods. The \textit{Jaccard} similarity between two documents is computed by dividing the size of the union of the corresponding term sets by the size of the intersection of them. This operation, particularly the operation of intersecting, is more costly than computing the \textit{cosine} similarity between vectors. Compared with \tfidf{}, \textit{Jaccard} costs the 20-fold, 10-fold and 21-fold operating time for \icontent{} over uni-, bi- and trigram, respectively.

\item The predicting phase consists of the representing phase and the phase of similarity computation. The first sub-phase depends on the complexity of the corresponding model. Specifically, a document is represented as a vector of the occurrence frequency, \tfidf{} weight or topic weight according to the applied STS method like BoW, \tfidf{} or topic models, and the computation complexity increases progressively. The second sub-phase depends on the dimension of the vector space and the amount of candidate vectors. Normally, the dimension of topic models is much lower than BoW and \tfidf{}, and hence, topic models cost less time in the second sub-phase. In \icontent{}, the cost of LSI is similar to BoW and \tfidf{}, whereas it is much higher than BoW and \tfidf{} in \ititle{} and \isummary{}, where the dimension of term-weighted vectors is lower due to the much smaller vocabularies. 

\end{itemize} 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/predicting_time}
    \caption[Average time cost (millisecond) of selection of related articles per target]{Average time cost (millisecond) of selection of related articles per target.}
    \label{fig:predict_time}
\end{figure}

In conclusion, we determine the final selection of the best STS methods for each processed field in table \ref{tab:select}, taking into account both of effectiveness and efficiency. Besides the methods with the best effectiveness, LSI is selected for applying in \icontent{} over unigram, because LSI outperforms \tfidf{} in metric \textit{nDCG} and \textit{precision@2@h} with a larger $h$, and moreover, it is selected for analyzing the characteristics of topic models. \tfidf{} is also selected for applying in \icontent{} over trigram, because \tfidf{} is much more efficient than Jaccard, although its effectiveness is slightly lower than Jaccard. 

\begin{table}[!htb]
\centering
\small
\begin{tabularx}{0.8\textwidth}{|X|X|X|X|}
\hline
\textbf{Model} & \textbf{Content} & \textbf{Title} & \textbf{Summary} \\ \hline
\textbf{Unigram} & \tfidf{}/LSI & \tfidf{} & \tfidf{} \\ \hline
\textbf{Bigram} & \tfidf{} & \tfidf{} & \textit{Jaccard} \\ \hline
\textbf{Trigram} & \textit{Jaccard}/\tfidf{} & \textit{Jaccard} & \textit{Jaccard} \\ \hline
\end{tabularx}
\caption[Selection of STS methods for each text fields over n-gram models.]{Selection of STS methods for each text fields over n-gram models. For \icontent{} over unigram, LSI is also selected because of its better performance in metic \textit{nDCG} and \textit{precision@2@h} with a larger $h$. Meanwhile, \tfidf{} is also chosen in \icontent{} over trigram because of its efficient with $21 \times$ speed compared with Jaccard in predicting phase} 
\label{tab:select}
\end{table}


\subsection{Results of Incremental Updating}
\label{sec:5.4}

As described in section \ref{sec:4.4}, the whole corpus is splitted into a historical dataset, in which the articles are released before 2012 and which is used for initializing the systems, and a target dataset containing the released after 2012 articles, which is used for evaluating the system in the selection of related articles and the system updating incrementally. We just take into account the first requirement of related articles for targets and hence we configure that the systems selects related articles which must be released before the target. The systems are evaluated in two aspects. On the one hand, we attempt to find the differences between the results of the system with incremental updating and the results of the controlled systems and analyze whether the differences are acceptable and the reason why the differences occur. On the other hand, we focus on the increasing trend in the time cost of updating and predicting along with increasing the corpus. 

Before analyzing the results, we setup the controlled systems in advance. The controlled systems keep constant once they are trained in the phase of initialization. In the first controlled systems, the STS methods in the system is trained from the initial candidate dataset. Accordingly, the systems contain only the semantic information which is collected before releasing the first target article, and this system is hence ``out-of-date''. Such systems are called the ``lower bound'' systems. In the second controlled systems, the only difference from the first systems is that the STS methods are trained by the all articles and the systems contain, therefore, the complete semantic information from the historical and future articles. The second controlled systems are named as the ``upper bound'' systems. Any article which is released before the current target is transformed into the corresponding representation by the STS methods.  The comparison between the evaluated systems and the controlled systems is able to indicate the particular characteristics of the systems with incremental updating and the differences from the ``lower-bound'' systems and ``upper-bound'' systems. 

Jaccard and BoW are not involved in the evaluation. These two methods are only dependent on the current articles and independent of the rest articles of the corpus. Therefore, it is not necessary to update the models during increasing the candidate corpus. Consequently, the evaluation concentrates on the VSMs, namely \tfidf{} and LSI (due to the high complexity and low performance, LDA is ignored). Based on the results of experiment 1, the systems in the processed field of \icontent{} over unigram always outperforms the systems using other processed fields. In this experiment, we only discuss the performance of the STS methods in \icontent{} with unigram. 

\subsubsection{Effectiveness}

Instead of \textit{precision@k@h} used in experiment 1, we depict the precision with moving average, which is often applied for handling with time series data to smooth out short-term fluctuations and highlight long-term trends or cycles. A typical application of moving average is to analyze stock prices. In our case, we calculate the moving mean precision (MMP) of the $i$-th target article and $n-1$ previous targets mathematically as follows:
\begin{equation}
    MM_{i} = \frac{\sum^{n-1}_{j=0} \text{correct predictions for }(i-j)}{k\times n}
\end{equation}

where $n$ is the moving window size and pre-defined as $100$ and $k$ is the number of related articles for each target and the default value thereof is $2$. 

Figure \ref{fig:predict_inc} illustrates the moving average precision of the evaluated systems and the controlled systems. In general, the average precision of the system using LSI and \tfidf{} is $29.95\%$ and $37.50\%$, respectively. The subfigures below are obtained by subtracting the precision of the controlled systems from the precision of the evaluated system. For method LSI, the evaluated system outperforms the ``lower bound'' system and underperforms the ``upper bound'' system definitely. Nevertheless, the differences in the systems using \tfidf{} are insignificant. Therefore, we observe that LSI is more sensitive than \tfidf{} to the completeness of information and model updating. Furthermore, for LSI, the difference between the evaluated system and the ``lower bound'' system keeps increasing along with handling more and more articles, and the difference between the evaluated system and the ``upper bound'' system is in the decreasing trend. 

The effectiveness of the system is dependent on the scale of the corpus, from which the system is trained. In the real world application, the judgements of related articles for each article can be updated every quarter or every half year, such that the system can discover related articles more precisely. 

\clearpage

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/precision_inc}
    \caption[Moving average precision of the evaluated system with the controlled systems]{Moving average precision of the evaluated system with the controlled systems. The window size of moving average is set to $400$. The upper sub-figures illustrate the precision plots of the evaluated, upper and lower systems with the color of blue, green and red, respectively. The bottom sub-figures show the plots of subtracting the precision of the upper and lower systems from the precision of the evaluated system, respectively.}
    \label{fig:predict_inc}
\end{figure}

\subsubsection{Efficiency}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig/runtime_inc}
    \caption[Efficiency of the evaluated system with incremental updating]{Efficiency of the evaluated system with incremental updating. The upper sub-figures show the time cost of building phase for each time of updating. The scatter plots are drawn with the practically consumed time (second) and the line plots are drawn by the linear regression to depict future time cost. The bottom sub-figures show the time cost of predicting phase. The plots with the color of blue are created by the practically consumed time and the plots with the color of bluish green are generated by the linear regression to depict the growth trends. }
    \label{fig:runtime_inc}
\end{figure}

Figure \ref{fig:runtime_inc} reports the time cost of building and predicting for the system using LSI and \tfidf{} respectively. The analysis of the reports is given as follows:

\begin{itemize}
    \item The time cost of the model building and the selection of related articles keeps linearly increasing during the candidate dataset keeps growing. For method LSI, the building time and predicting time increase by $19.3$ seconds and $5.8$ milliseconds, respectively, with the size of candidates increased by $1000$. Meanwhile, for method \tfidf{}, the building time and the predicting time increase by $4.0$ seconds and $3.5$ milliseconds, respectively. LSI is slower than \tfidf{} in both of building and predicting. 
    \item The jitter of predicting time is caused by the implementation. As an article is stored in the system, the system puts it at first into a temporary collection. The temporary collection will be merged into the candidate dataset. The similarity computation with articles in the candidate dataset is much more efficient than the computation of similarity with articles in the temporary collection. That is why the predicting time increases significantly before next updating and decreases to the normal level after updating. 
    
\end{itemize}

\bigbreak

\subsection{Error Analysis}
\label{sec:5.5}

We present insights on the most types of errors of false positive (fp) and false negative (fn) that occurred in the experiments. In order to confirm the reasons or conditions of the errors, we inspect manually the predictions of the framework to discover related articles and compare them with the golden standard understanding of human beings.  

\subsubsection{False Positive Error}

A false positive error is a result that indicates a given condition has been fulfilled, when it actually has not been fulfilled. In our case, a false positive error is the event that an article which is actually unrelated to the target article is assigned as related by the framework. 

\begin{itemize}
    \item One type of fp errors is caused by the labeled corpus. The list of related articles is created by the author manually and the size is limited up to $2$. The list is incomplete, so that articles which should be related to each other under human understanding are unreachable to each other in the related-graph. Only this type is not the real errors but the inherent systematic lack. 
    
    Example: 
    \begin{itemize}
        \item \textit{``Tod dem Diktator'' - Rufe zur Vereidigung} (\textit{``Death to the dictator'' - require for swearing}) \footnote{\url{http://www.zeit.de/online/2009/32/iran-proteste-vereidigung}}
        \item \textit{Proteste in Teheran dauern an} (\textit{Protests in Tehran persist}) \footnote{\url{http://www.zeit.de/online/2009/32/iran-ahmadineschad-proteste}}
    \end{itemize}
    The both articles report the protest against Mahmoud Ahmadinejad, who is re-elected as the Iranian president, and the international as well as authoritative reaction. The articles should be related by the human judgement, but there is no related-path between them. We consider that the incompleteness of labeling causes the fp error. 
    
    \item One article usually contains a set of topics rather than the single topic. The topics are not treated equally, but they are understood in the different importance. It is complicated and subtle, how human beings weigh the importance of different topics. From such viewpoint, two articles are assigned as related, only when they share the topics with the highest weight, otherwise they are unrelated, even though they are similar to each other in semantic or lexical meaning. 
    
    Example:
    \begin{itemize}
        \item \textit{Tunesiens Innenminister wird neuer Regierungschef} (\textit{Tunisia Interior Minister appointed new head of government}) \footnote{\url{http://www.zeit.de/politik/ausland/2013-02/tunesien-regierungschef-larayedh-jebali}}
        \item \textit{Tunesien hebt den Ausnahmezustand seit Arabellion auf} (\textit{Tunisia cancels the state of emergency beginning from Arabellion}) \footnote{\url{http://www.zeit.de/politik/ausland/2014-03/tunesien-ausnahmezustand-aufhebung}}
    \end{itemize}
    
    The both articles mention the identical background, subject of the events and keywords. However, the critical reported topics are different and unrelated. In addition, quite a few named entities which they share confuse the framework predicts the relatedness between them. 
    
    \item Two articles have the identical or similar information, but they mention the very different topics. 
    Example: 
    \begin{itemize}
        \item \textit{Dem Pr\"asidentenpaar droht eine herbe Wahlniederlage} (\textit{The presidential couple threatens a bitter election defeat}) \footnote{\url{http://www.zeit.de/online/2009/27/argentinien-wahl-kirchner}}
        \item \textit{Der Schattenpr\"asident} (\textit{The Shadow President}) \footnote{\url{http://www.zeit.de/politik/ausland/2010-10/nestor-kirchner-nachruf}}
    \end{itemize}
    The articles report the politics in Argentina and use many similar or same words to describe the situation of the government and the action thereof. However, the first article focuses on the difficulty of election, while the second article reviews mainly the record of achievements of the former president who just died. Hence, they are unrelated very clearly. 
    
    \item Two articles are about entirely different field and totally unrelated. 
    
    Example: 
    \begin{itemize}
        \item \textit{Ohne Gras kein Spa\ss{}} (\textit{Without grass no fun}) \footnote{\url{http://www.zeit.de/2009/20/Portugal-Golf}}
        \item \textit{Jonglieren entspannt und macht schlau} (\textit{Juggling relaxed and makes you smart}) \footnote{\url{http://www.zeit.de/karriere/beruf/2012-10/konzentration-gehirnleistung-jonglage}}
    \end{itemize}
    
    The first article describes the experiences of the golf trip in Portugal, whereas the second article reports a kind of game in office which can make the participants relaxer and smarter. They belong to even different separated categories and cannot be related to each other. 
\end{itemize}

\subsubsection{False Negative Error}

A false positive error is, in our case, a result that indicates an article which is actually quite related to the target article is judged with a very low relatedness degree. Now we consider that the articles which are very related to the target article are exactly predicted unrelated with an unexpected low relatedness degree. More specifically, we only inspect the extreme and typical error that the framework treats the articles which are directly adjacent to the target article in the related-graph and should be one of the most related articles as unrelated in a very low level. We discuss the characteristics of the errors and the reasons why the errors occur. 

\begin{itemize}
    \item Two articles discuss the same topics and related meaning, but the points of view and concrete events are different. 
    
    Example:
    \begin{itemize}
    \item \textit{Drittmittel sind ungleich verteilt} (\textit{Third-party funds are distributed unequally}) \footnote{\url{http://www.zeit.de/studium/hochschule/2014-02/Drittmittel-an-Hochschulen}}
    \item \textit{Uni Leipzig droht mit Schlie\ss{}ung ganzer Fakult\"aten} (\textit{Uni Leipzig threatened with closure of entire faculties}) \footnote{\url{http://www.zeit.de/studium/hochschule/2014-02/universitaet-leipzig-finanzierung}}
    \end{itemize}
    The first article discusses the phenomenon of education abstractly, while the second reports the very concrete expression in a specific university. They are not similar semantically, but related because of the same topic.
    
    
    \item Humans classify an articles into topics hierarchically rather than separately. Two articles may be related in higher level but different in the particular level. The critical point is how the humans determine which level is the most important. 
    
    Example:
    \begin{itemize}
        \item \textit{Der Jugendschutzfilter blockiert zu viel} (\textit{The youth safety filter blocking too much}) \footnote{\url{http://www.zeit.de/digital/internet/2012-02/jugendschutzfilter-filtern-blogs}}
        \item \textit{Filmwirtschaft weitet Selbstkontrolle auf Internet aus} (\textit{Film industry is expanding self-regulation on the Internet}) \footnote{\url{http://www.zeit.de/kultur/film/2011-10/fsk-jugendschutz-internet}}
    \end{itemize}    
    
    The first article reports the current situation of the youth safety filter and the second article reports that the change of film industry on the Internet may harm the youth protection. These two article share the same topic on ``youth procection'' but have difference on the particular fields. They are labeled as related based on the editor's understanding. However, this judgement is not absolutely correct. 
    
\end{itemize}

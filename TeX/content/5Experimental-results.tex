%!TEX root = <../index.tex>

\section{Experimental Results}


In this chapter, we report the results from the evaluation experiment. We mark a number of articles as target articles randomly and find their related articles in the entire corpus. In the experiments, we choose 1000 target articles from 29787 articles as the corpus. The experiment consists of four phases: preprocessing raw data, reducing candidate, calculating similarity score and selecting articles with satisfied score or ranking. In the following sections the results of evaluation of the four components are given. 

In ZEIT-corpus there is for each article a list of recommended articles, that were edited by the author manually. The corpus is treated as a undirected graph, in which articles are regarded as vertices and the recommendation-relationships are edges between target article and the recommended one. Typically the recommended articles are published earlier than the target. In the view of graph theory, we define the term \textbf{related}. If a path between two articles exists and the length thereof is shorted than the pre-defined maximal value (by default: 3), the two articles can be treated as \textbf{related} to each other. The goal of finding related articles is that recommendations are provided automatically to readers instead of the manual work. For this purpose, the system selects the top $k$ articles as recommendation. \textbf{precision} is the most valuable and reasonable measure of evaluation in this scenario. Formally, $G_v^n$ refers to a list of vertices, in which the path between any vertex and vertex $v$ is shorter than $n$, that is also the so-called hops. $R_v=\{d_1, \cdots, d_k\}$ is the prediction of the system given by vertex $v$. The set of target articles is signed as $T$. The precision equals:
$$\frac{\sum_{v\in T}|R_v \cap G_v^n|}{k \cdot |T|}$$


\subsection{Preprocessing Selection}

An articles in the corpus is stored in string in rich text format (HTML). Before building semantic similarity models, the string is splitted into an ordered list of tokens with a preprocessing approach. In this case, four preprocessing approaches are evaluated. First, the simplest approach is to remain all elements in the original token list, called \textbf{splitword}. The second approach \textbf{split\_nocase} is to ignore the case sensitivity of tokens. Furthermore, \textbf{stopword} is the third choice of preprocessing, in which the given stopwords (specified by existed knowledge given language) or the most common (e.g. 100) words in the entire corpus are removed. The last one, \textbf{stem}, is that only the stems of tokens are remained after the common words are filtered by \textbf{stopword}. 

The performance of using different preprocessing approaches, the vocabulary size and the number of terms that occur at least 5 times in the entire corpus can be seen in Table \ref{preprocess_result}. The constant settings of the system is (model=bag-of-word, hops=3).

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{approach}               & \textbf{\begin{tabular}[c]{@{}l@{}}vocabulary\\ size\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}vocabulary\\ size (min5)\end{tabular}} & \textbf{precision} \\ \hline
\textit{\textbf{splitword}}     & 616,012                                                             & 274,323                                                                    & 0.175              \\ \hline
\textit{\textbf{split\_nocase}} & 584,868                                                             & 260,368                                                                    & 0.167              \\ \hline
\textit{\textbf{stopword}}      & 584,642                                                             & 260,143                                                                    & 0.413              \\  \hline
\textit{\textbf{stem}}          & \textbf{361,605}                                                    & \textbf{175,215}                                                           & \textbf{0.452} \\     \hline
\end{tabular}
\caption{Performance across different preprocessing approaches and the vocabulary size respectively.}
\label{preprocess_result}
\end{table}

As shown in Table \ref{preprocess_result}, the bag-of-word model combined with \textbf{stem} approach has the best performance and the smallest vocabulary size. Compared to the vocabulary generated by \textbf{splitword}, the size of the complete and the min-5 vocabulary from \textbf{stem} is reduced by 41.3\% and 36.1\% respectively. In the second place is \textbf{stopword}. However, the gab of performance and the vocabulary size therebetween are not significant. In the following experiments, in the preprocessing phase the \textbf{stem} is applied by default, and \textbf{stopwrod} is also used as comparable group. 

\subsection{Coverage in Relation-Graph}

In this section, we evaluate how the recommendation of related articles from predictors covers the subgraph of the target article. The position of recommendations in the graph is important. A predictor, by which given recommendations are in the subgraph of the target article, is obviously better than a predictor which predicts the absolute unrelated articles as recommendation. Moreover, it will be weaker than a predictor, from which recommended articles are situated nearer the target article. 

At first, we make an assumption, that the radius of the relationship subgraph of a given article is at most 10, i.e. articles which are reachable from the target article over at most 10 hops, are treated as (weak) related articles to the target. In the table \ref{cover_graph_result} is shown, how the precision of different semantic similarity models in the different-sized relationship-graph looks like. 


\begin{table}[!htbp]
\centering
\begin{tabular}{|r|r|r|r|}
\hline
\multicolumn{1}{|l|}{\textbf{hops}} & \multicolumn{1}{|l|}{\textbf{avg. graph size}} & \multicolumn{1}{|l|}{\textbf{\begin{tabular}[|c|]{@{}l@{}}random\\ precision\end{tabular}}} & \multicolumn{1}{|l|}{\textbf{\begin{tabular}[|c|]{@{}l@{}}bag-of-words\\ precision\end{tabular}}} \\

\hline 
\textbf{1}    & 3.91                & 0.13 e-3                  & 0.209                          \\ \hline
\textbf{2}    & 15.55               & 0.52 e-3                  & 0.337                          \\ \hline
\textbf{3}    & 43.16               & 0.145 e-2                 & 0.413                          \\ \hline
\textbf{4}    & 107.58              & 0.361 e-2                 & 0.485                          \\ \hline
\textbf{5}    & 254.05              & 0.853 e-2                 & 0.553                          \\ \hline
\textbf{6}    & 569.12              & 0.191 e-1                 & 0.617                          \\ \hline
\textbf{7}    & 1,215.29            & 0.408 e-1                 & 0.674                          \\ \hline
\textbf{8}    & 2,453.72            & 0.824 e-1                 & 0.730                          \\ \hline
\textbf{9}    & 4,640.66            & 0.156                     & 0.780                          \\ \hline
\textbf{10}   & 8,062.86            & 0.271                     & 0.834                         \\ \hline
\end{tabular}
\caption{Precision in different-sized relationship-graph}
\label{cover_graph_result}
\end{table}

In the table \ref{cover_graph_result} all of the models have much better performance than the random selection. That is to say, the semantic similarity methods are efficient. 

\subsection{Model Selection}

We introduce the different semantic similarity models in section~\ref{sec:2}. In this section, we make the experiments to evaluate the performance of the system across two dimensions models and preprocessing approaches. The candidate of preprocessing approaches are \textbf{stopword} and \textbf{stem}, as discussed in section 5.1. The semantic similarity models are categorised into string based methods and vector space based methods. The string based methods include \textbf{(1, 2, 3)-gram jaccard index}, \textbf{LCS}, and \textbf{greedy tiling string} (only \textbf{split\_nocase} is suitable to the last two approaches), while the vector space based methods are \textbf{bag-of-words}(baseline), \textbf{tf-idf}, \textbf{lsi} and \textbf{lda}.


\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|l|l|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{model}} & \textbf{preprocess} & \multicolumn{1}{c|}{\textbf{bset-2}} & \multicolumn{1}{c|}{\textbf{best-5}} & \multicolumn{1}{c|}{\textbf{best-10}} & \multicolumn{1}{c|}{\textbf{best-20}} & \multicolumn{1}{c|}{\textbf{best-50}} & \multicolumn{1}{c|}{\textbf{best-100}} \\ \hline
\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}string\\ based\\ model\end{tabular}}} & \multirow{2}{*}{\textbf{1gram-jaccard}} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{3-9} 
 &  & \textbf{stem} &  &  &  &  &  &  \\ \cline{2-9} 
 & \multirow{2}{*}{\textbf{2gram-jaccard}} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{3-9} 
 &  & \textbf{stem} &  &  &  &  &  &  \\ \cline{2-9} 
 & \textbf{3gram-jaccard} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{2-9} 
 & \textbf{lcs} & \textbf{split\_nocase} &  &  &  &  &  &  \\ \hline
\multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}vector\\ space\\ model\end{tabular}}} & \multirow{2}{*}{\textbf{bag-of-words}} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{3-9} 
 &  & \textbf{stem} &  &  &  &  &  &  \\ \cline{2-9} 
 & \multirow{2}{*}{\textbf{tf-idf}} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{3-9} 
 &  & \textbf{stem} &  &  &  &  &  &  \\ \cline{2-9} 
 & \multirow{2}{*}{\textbf{lsi}} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{3-9} 
 &  & \textbf{stem} &  &  &  &  &  &  \\ \cline{2-9} 
 & \multirow{2}{*}{\textbf{lda}} & \textbf{stopword} &  &  &  &  &  &  \\ \cline{3-9} 
 &  & \textbf{stem} &  &  &  &  &  &  \\ \hline
\end{tabular}
\caption{Precision of different models}
\label{different_models}
\end{table}

Tf-idf model has the best Performance. LSI model is in the second place. 

\subsection{Divergence in Categories}

In this section we will discuss a predicament of prediction and evaluation. 

Semantic Textual Similarity is a relatively subjective judgement of individual, and also depends on the field of the articles. The relationship between topic fields can be depicted simply in an Euler diagram. In the diagram, the biggest concepts are the categories of articles, where are treated as top level field. Within these categories, there are many levels of fields, of which intersection exists probably. We discuss an example to explain it. The system predicts two articles related to each other. One of them reported the football game of Euro Champions between FC Bayern and Real Madrid in 2014, while the other reported the football game of German Bundesliga between FC Bayern and Dortmund in 2015. In the ZEIT corpus, the two articles are unrelated, intuitively because these two games are in different seasons and different leagues and the reports of football games are relatively short-term. The statistic is followed in Table \ref{precision_category}, to indicate how suitable the system are for different categories.

As in the table \ref{precision_category} depicted, the system has the best performance in Wissen and worst in Kultur. 

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|l|l|l|l|}
\hline
\textbf{Category} & \textbf{Best-2} & \textbf{Best-5} & \textbf{Best-10} & \textbf{Best-20} & \textbf{Best-50} & \textbf{Best-100} \\ \hline
Wissen            & 0.544           & 0.435           &                  &                  &                  &                   \\ \hline
Politik           & 0.512           & 0.429           &                  &                  &                  &                   \\ \hline
Gesellschaft      & 0.496           & 0.432           &                  &                  &                  &                   \\ \hline
%\rowcolor[HTML]{C0C0C0} 
{\ul All}         & {\ul 0.452}     & {\ul 0.379}     & {\ul }           & {\ul }           & {\ul }           & {\ul }            \\ \hline
Sport             & 0.444           & 0.361           &                  &                  &                  &                   \\ \hline
Wirtschaft        & 0.374           & 0.317           &                  &                  &                  &                   \\ \hline
Kultur            & 0.282           & 0.240           &                  &                  &                  &                   \\ \hline
\end{tabular}
\caption{Precision by different categories. [3 hops]}
\label{precision_category}
\end{table}

\[Conclusion: TBD\] 

\subsection{Meta-data}

The ZEIT corpus provides three kinds of meta-data for each article: category, release date and keywords list. The characteristic of each meta-data and influence of utilizing the meta-data into the system are introduced in the following chapter.

\subsubsection{Category}

The distribution of the categories of related articles by given target articles is shown in the followed table. For each target article, the system considers only the articles with the categories of which ratio are greater than the given threshold (by default 0.05). That has an obvious advantage, that the amount of candidate to evaluate are reduced impressively (~50\%). From the table \ref{diff_filter_cate}, the performance of the base system and the variety with reducing candidate using the information of categories is slightly and even negligibly different in most models (except LDA model).   

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|l|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{model}} & \textbf{preprocess} & \textbf{base} & \textbf{filter} & \textbf{diff} \\ \hline
\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}string\\ based\\ model\end{tabular}}} & \multirow{2}{*}{\textbf{1gram-jaccard}} & \textbf{stopword} & 0.427 & 0.424 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.431 & 0.427 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{2gram-jaccard}} & \textbf{stopword} & 0.393 & 0.399 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.429 & 0.427 &  \\ \cline{2-6} 
 & \textbf{3gram-jaccard} & \textbf{stopword} & 0.290 & 0.291 &  \\ \cline{2-6} 
 & \textbf{lcs} & \textbf{split\_nocase} & 0.193 &  &  \\ \hline
\multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}vector\\ space\\ model\end{tabular}}} & \multirow{2}{*}{\textbf{bag-of-words}} & \textbf{stopword} & 0.413 & 0.411 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.452 & 0.447 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{tf-idf}} & \textbf{stopword} & 0.452 & 0.448 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.469 & 0.465 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{lsi}} & \textbf{stopword} & 0.419 & 0.432 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.444 & 0.441 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{lda}} & \textbf{stopword} & 0.130 & 0.223 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.193 & 0.242 &  \\ \hline
\end{tabular}
\label{diff_filter_cate}
\end{table}

\subsubsection{Release Date}


\subsubsection{Keywords}


$$ Sim_{i, j}^{new} = Sim_{i, j}^{old} \cdot (1 + 0.1 \cdot |(keywords(i) \cap keywords(j))|)$$

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|l|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{model}} & \textbf{preprocess} & \multicolumn{1}{c|}{\textbf{base}} & \multicolumn{1}{c|}{\textbf{keywords}} & \multicolumn{1}{c|}{\textbf{diff}} \\ \hline
\multirow{6}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}string\\ based\\ model\end{tabular}}} & \multirow{2}{*}{\textbf{1gram-jaccard}} & \textbf{stopword} & 0.427 & 0.430 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.431 & 0.429 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{2gram-jaccard}} & \textbf{stopword} & 0.393 & 0.439 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.429 & 0.455 &  \\ \cline{2-6} 
 & \textbf{3gram-jaccard} & \textbf{stopword} & 0.290 & 0.342 &  \\ \cline{2-6} 
 & \textbf{lcs} & \textbf{split\_nocase} & 0.193 &  &  \\ \hline
\multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}vector\\ space\\ model\end{tabular}}} & \multirow{2}{*}{\textbf{bag-of-words}} & \textbf{stopword} & 0.413 & 0.435 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.452 & 0.455 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{tf-idf}} & \textbf{stopword} & 0.452 & 0.470 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.469 & 0.469 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{lsi}} & \textbf{stopword} & 0.419 & 0.439 &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.444 & 0.444 &  \\ \cline{2-6} 
 & \multirow{2}{*}{\textbf{lda}} & \textbf{stopword} & 0.130 &  &  \\ \cline{3-6} 
 &  & \textbf{stem} & 0.193 &  &  \\ \hline
\end{tabular}
\label{addition_keywords}
\end{table}

\subsection{Utilization of (Super-, Sub-)Title and Summary Paragraph}


























